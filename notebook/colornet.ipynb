{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Importing Dataset: Stable-ImageNet1K\n",
    "\n",
    "We will be using the **Stable-ImageNet1K** dataset, which contains:\n",
    "\n",
    "- **100 images** from each of **1,000 different classes**\n",
    "- A total of **100,000 images**\n",
    "- Approximately **10.5 GB** in size\n",
    "\n",
    "> ‚è≥ Note: Downloading this dataset may take a while due to its size ‚Äî but once it's cached, future accesses will be much faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/stable-imagenet1k\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"vitaliykinakh/stable-imagenet1k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Importing Necessary Libraries\n",
    "\n",
    "Before we begin, let's import the essential libraries required for loading the dataset, preprocessing, and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T16:58:41.011417Z",
     "iopub.status.busy": "2025-05-03T16:58:41.011239Z",
     "iopub.status.idle": "2025-05-03T16:58:41.015237Z",
     "shell.execute_reply": "2025-05-03T16:58:41.014373Z",
     "shell.execute_reply.started": "2025-05-03T16:58:41.011401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:33:05.731261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746383586.216822      71 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746383586.348749      71 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Loading Image Paths\n",
    "\n",
    "We will now load all image file paths from the dataset directory.  \n",
    "Only files with the `.jpg` extension will be considered, ensuring we include only valid image files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T16:58:41.016798Z",
     "iopub.status.busy": "2025-05-03T16:58:41.016140Z",
     "iopub.status.idle": "2025-05-03T16:58:54.533390Z",
     "shell.execute_reply": "2025-05-03T16:58:54.532650Z",
     "shell.execute_reply.started": "2025-05-03T16:58:41.016773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_paths = glob.glob('/kaggle/input/stable-imagenet1k/imagenet1k/*/*.jpg') \n",
    "print(f\"Total images: {len(image_paths)}\")\n",
    "\n",
    "image_paths = tf.constant(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßπ Preprocessing\n",
    "\n",
    "Since we are using an **autoencoder architecture**, our goal is to reconstruct the original color image from a simplified input.  \n",
    "To achieve this, we:\n",
    "\n",
    "- Convert the **ground truth color images** into a set of **grayscale images** to serve as the input to the autoencoder.\n",
    "- This allows the model to **learn the mapping from grayscale to color**, effectively performing image colorization.\n",
    "\n",
    "> üìå This setup is ideal for learning meaningful representations, especially in tasks like unsupervised feature learning or image-to-image translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T16:58:54.535008Z",
     "iopub.status.busy": "2025-05-03T16:58:54.534770Z",
     "iopub.status.idle": "2025-05-03T16:58:54.539150Z",
     "shell.execute_reply": "2025-05-03T16:58:54.538669Z",
     "shell.execute_reply.started": "2025-05-03T16:58:54.534990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "\n",
    "def decode_and_preprocess(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    gray = tf.image.rgb_to_grayscale(img)\n",
    "    return gray, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T17:01:52.804127Z",
     "iopub.status.busy": "2025-05-03T17:01:52.803841Z",
     "iopub.status.idle": "2025-05-03T17:01:52.840201Z",
     "shell.execute_reply": "2025-05-03T17:01:52.839435Z",
     "shell.execute_reply.started": "2025-05-03T17:01:52.804106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure image_paths is a plain list\n",
    "image_paths = image_paths.numpy().tolist() if hasattr(image_paths, 'numpy') else list(image_paths)\n",
    "\n",
    "# Now split\n",
    "train_paths, val_paths = train_test_split(image_paths, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Constructing Training and Validation Sets\n",
    "\n",
    "We split our dataset into:\n",
    "\n",
    "- A **training set** for learning the autoencoder weights.\n",
    "- A **validation set** to monitor performance and prevent overfitting during training.\n",
    "\n",
    "We do **not use a separate test set**, as this is an **unsupervised learning task**.  \n",
    "Since we are not evaluating against labeled targets, our focus is on how well the model reconstructs input images.\n",
    "\n",
    "> ‚ö†Ô∏è A test set could still be used for qualitative evaluation or metric-based comparison (e.g., PSNR or SSIM), but it is not strictly necessary in this context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T17:02:09.017588Z",
     "iopub.status.busy": "2025-05-03T17:02:09.016885Z",
     "iopub.status.idle": "2025-05-03T17:02:09.298946Z",
     "shell.execute_reply": "2025-05-03T17:02:09.298189Z",
     "shell.execute_reply.started": "2025-05-03T17:02:09.017564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "def decode_and_preprocess(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    gray = tf.image.rgb_to_grayscale(img)\n",
    "    return gray, img\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "train_dataset = train_dataset.shuffle(10000).map(decode_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(val_paths)\n",
    "val_dataset = val_dataset.map(decode_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Defining PSNR and SSIM\n",
    "\n",
    "To evaluate the performance of our autoencoder in reconstructing images, we use two widely adopted image quality metrics:\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÆ Peak Signal-to-Noise Ratio (PSNR)\n",
    "\n",
    "**PSNR** measures the ratio between the maximum possible power of a signal (image) and the power of corrupting noise (error introduced by reconstruction).  \n",
    "It is expressed in decibels (dB), where a **higher PSNR indicates better reconstruction quality**.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{PSNR} = 10 \\cdot \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)\n",
    "$$\n",
    "\n",
    "- **MAX** is the maximum possible pixel value of the image (e.g., 255 for 8-bit images)\n",
    "- **MSE** is the Mean Squared Error between the original and reconstructed image:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{mn} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\left[ I(i,j) - K(i,j) \\right]^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I(i, j) &\\text{ is the pixel value of the original image} \\\\\n",
    "K(i, j) &\\text{ is the pixel value of the reconstructed image}\n",
    "\\end{aligned}\n",
    "$$\n",
    "---\n",
    "\n",
    "#### üß† Structural Similarity Index Measure (SSIM)\n",
    "\n",
    "**SSIM** evaluates image similarity based on perceptual factors like luminance, contrast, and structure.  \n",
    "Unlike PSNR, which is purely pixel-based, **SSIM better aligns with human visual perception**.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_x,\\ \\mu_y &\\text{ are the means of images } x \\text{ and } y \\\\\n",
    "\\sigma_x^2,\\ \\sigma_y^2 &\\text{ are the variances of images } x \\text{ and } y \\\\\n",
    "\\sigma_{xy} &\\text{ is the covariance between } x \\text{ and } y \\\\\n",
    "C_1,\\ C_2 &\\text{ are constants to stabilize the division}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_1 = (K_1 L)^2, \\quad C_2 = (K_2 L)^2\n",
    "$$\n",
    "\n",
    "with \\( L \\) being the dynamic range of pixel values (e.g., 255), and \\( K_1 = 0.01, K_2 = 0.03 \\)\n",
    "\n",
    "**Range:** SSIM values range from **-1 to 1**, where:  \n",
    "- **1** indicates perfect structural similarity  \n",
    "- **0 or less** implies no similarity\n",
    "\n",
    "---\n",
    "\n",
    "> ‚úÖ **In summary**:  \n",
    "> - **PSNR** gives a straightforward signal fidelity measure (good for optimization).  \n",
    "> - **SSIM** offers a more perceptual quality assessment (good for image similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T16:58:54.735585Z",
     "iopub.status.busy": "2025-05-03T16:58:54.735390Z",
     "iopub.status.idle": "2025-05-03T16:58:54.739385Z",
     "shell.execute_reply": "2025-05-03T16:58:54.738576Z",
     "shell.execute_reply.started": "2025-05-03T16:58:54.735569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def psnr(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "    \n",
    "@tf.keras.utils.register_keras_serializable()    \n",
    "def ssim(y_true, y_pred):\n",
    "    return tf.image.ssim(y_true, y_pred, max_val=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Autoencoder Architecture Overview\n",
    "\n",
    "The `build_autoencoder` function defines a **convolutional autoencoder** that learns to map grayscale images to RGB color images. The architecture consists of three main parts:\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ Encoder\n",
    "\n",
    "The encoder progressively downsamples the input using convolutional layers with **LeakyReLU activations**, **batch normalization**, and **max pooling**:\n",
    "\n",
    "- Input shape: \\( (128, 128, 1) \\) ‚Äî a grayscale image\n",
    "- Three encoding blocks:\n",
    "  - Conv2D ‚Üí BatchNorm ‚Üí LeakyReLU ‚Üí MaxPooling\n",
    "  - Feature depth increases from 64 ‚Üí 128 ‚Üí 256\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ Bottleneck\n",
    "\n",
    "A deeper convolutional layer acts as the **bottleneck**, where the image is represented in a compressed feature space:\n",
    "\n",
    "- Conv2D ‚Üí BatchNorm ‚Üí LeakyReLU\n",
    "- Output depth: 512 filters\n",
    "- No pooling here ‚Äî acts as the \"compressed latent representation\"\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ Decoder\n",
    "\n",
    "The decoder upsamples the compressed features to reconstruct the full-resolution output:\n",
    "\n",
    "- Three decoding blocks:\n",
    "  - UpSampling ‚Üí Concatenate (skip connection) ‚Üí Conv2D ‚Üí BatchNorm ‚Üí LeakyReLU\n",
    "  - Skip connections from encoder layers preserve spatial detail\n",
    "  - Feature depth decreases from 256 ‚Üí 128 ‚Üí 64\n",
    "\n",
    "---\n",
    "\n",
    "#### üéØ Output Layer\n",
    "\n",
    "- Final `Conv2D` layer with **3 output channels** (RGB)\n",
    "- Uses `sigmoid` activation to produce pixel values in the \\([0, 1]\\) range\n",
    "\n",
    "---\n",
    "\n",
    "> ‚úÖ This architecture is well-suited for **image colorization**, as it learns to reconstruct full-color images from grayscale inputs using both global and local features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T16:58:54.740282Z",
     "iopub.status.busy": "2025-05-03T16:58:54.740037Z",
     "iopub.status.idle": "2025-05-03T16:58:54.755697Z",
     "shell.execute_reply": "2025-05-03T16:58:54.755117Z",
     "shell.execute_reply.started": "2025-05-03T16:58:54.740258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_autoencoder(img_size=128):\n",
    "    input_img = Input(shape=(img_size, img_size, 1))\n",
    "\n",
    "    # Encoder\n",
    "    x1 = Conv2D(64, (3,3), padding='same')(input_img)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = LeakyReLU()(x1)\n",
    "    p1 = MaxPooling2D((2,2))(x1)\n",
    "\n",
    "    x2 = Conv2D(128, (3,3), padding='same')(p1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = LeakyReLU()(x2)\n",
    "    p2 = MaxPooling2D((2,2))(x2)\n",
    "\n",
    "    x3 = Conv2D(256, (3,3), padding='same')(p2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = LeakyReLU()(x3)\n",
    "    p3 = MaxPooling2D((2,2))(x3)\n",
    "\n",
    "    # üîπ Bottleneck\n",
    "    bn = Conv2D(512, (3,3), padding='same')(p3)\n",
    "    bn = BatchNormalization()(bn)\n",
    "    bn = LeakyReLU()(bn)\n",
    "\n",
    "    # Decoder\n",
    "    u3 = UpSampling2D((2,2))(bn)\n",
    "    u3 = Concatenate()([u3, x3])\n",
    "    x4 = Conv2D(256, (3,3), padding='same')(u3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "\n",
    "    u2 = UpSampling2D((2,2))(x4)\n",
    "    u2 = Concatenate()([u2, x2])\n",
    "    x5 = Conv2D(128, (3,3), padding='same')(u2)\n",
    "    x5 = BatchNormalization()(x5)\n",
    "    x5 = LeakyReLU()(x5)\n",
    "\n",
    "    u1 = UpSampling2D((2,2))(x5)\n",
    "    u1 = Concatenate()([u1, x1])\n",
    "    x6 = Conv2D(64, (3,3), padding='same')(u1)\n",
    "    x6 = BatchNormalization()(x6)\n",
    "    x6 = LeakyReLU()(x6)\n",
    "\n",
    "    # Output layer (RGB)\n",
    "    output = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x6)\n",
    "\n",
    "    model = Model(inputs=input_img, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T17:02:14.630223Z",
     "iopub.status.busy": "2025-05-03T17:02:14.629925Z",
     "iopub.status.idle": "2025-05-03T17:02:14.637073Z",
     "shell.execute_reply": "2025-05-03T17:02:14.636320Z",
     "shell.execute_reply.started": "2025-05-03T17:02:14.630202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy() #This is used because we ran on Kaggle with 2 GPUs\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:06:51.920637Z",
     "iopub.status.busy": "2025-05-03T18:06:51.919916Z",
     "iopub.status.idle": "2025-05-03T18:06:51.924118Z",
     "shell.execute_reply": "2025-05-03T18:06:51.923496Z",
     "shell.execute_reply.started": "2025-05-03T18:06:51.920611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=3,              \n",
    "    restore_best_weights=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:06:54.473439Z",
     "iopub.status.busy": "2025-05-03T18:06:54.472604Z",
     "iopub.status.idle": "2025-05-03T18:06:54.705966Z",
     "shell.execute_reply": "2025-05-03T18:06:54.705201Z",
     "shell.execute_reply.started": "2025-05-03T18:06:54.473397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    autoencoder = build_autoencoder()\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=[psnr, ssim])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T18:07:00.904003Z",
     "iopub.status.busy": "2025-05-03T18:07:00.903670Z",
     "iopub.status.idle": "2025-05-03T19:19:36.811540Z",
     "shell.execute_reply": "2025-05-03T19:19:36.810937Z",
     "shell.execute_reply.started": "2025-05-03T18:07:00.903980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T20:06:15.576322Z",
     "iopub.status.busy": "2025-05-03T20:06:15.575612Z",
     "iopub.status.idle": "2025-05-03T20:06:15.724028Z",
     "shell.execute_reply": "2025-05-03T20:06:15.723261Z",
     "shell.execute_reply.started": "2025-05-03T20:06:15.576296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "autoencoder.save('ColorNet.keras')\n",
    "dump(autoencoder, 'ColorNet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Training and Validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T19:33:37.054654Z",
     "iopub.status.busy": "2025-05-03T19:33:37.054203Z",
     "iopub.status.idle": "2025-05-03T19:33:37.536712Z",
     "shell.execute_reply": "2025-05-03T19:33:37.535977Z",
     "shell.execute_reply.started": "2025-05-03T19:33:37.054632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0.008, 0.02)  \n",
    "\n",
    "\n",
    "if 'psnr' in history.history:\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['psnr'], label='Train PSNR')\n",
    "    if 'val_psnr' in history.history:\n",
    "        plt.plot(history.history['val_psnr'], label='Val PSNR')\n",
    "    plt.title('PSNR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.legend()\n",
    "    plt.ylim(12, 23)\n",
    "    \n",
    "if 'ssim' in history.history:\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['ssim'], label='Train SSIM')\n",
    "    if 'val_ssim' in history.history:\n",
    "        plt.plot(history.history['val_ssim'], label='Val SSIM')\n",
    "    plt.title('SSIM')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('SSIM')\n",
    "    plt.legend()\n",
    "    plt.ylim(0.85, 0.92)  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T19:24:19.286481Z",
     "iopub.status.busy": "2025-05-03T19:24:19.285941Z",
     "iopub.status.idle": "2025-05-03T19:24:19.369184Z",
     "shell.execute_reply": "2025-05-03T19:24:19.368631Z",
     "shell.execute_reply.started": "2025-05-03T19:24:19.286461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "test_batch = next(iter(dataset.take(1)))\n",
    "grayscale_imgs, real_color_imgs = test_batch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(val_dataset.take(2)))\n",
    "grayscale_imgs, real_color_imgs = test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T19:24:20.640590Z",
     "iopub.status.busy": "2025-05-03T19:24:20.640130Z",
     "iopub.status.idle": "2025-05-03T19:24:20.893757Z",
     "shell.execute_reply": "2025-05-03T19:24:20.893221Z",
     "shell.execute_reply.started": "2025-05-03T19:24:20.640568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predicted_color_imgs = autoencoder.predict(grayscale_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Visualizing Reconstruction Results\n",
    "\n",
    "To qualitatively assess the performance of our autoencoder, we can visualize:\n",
    "\n",
    "- The **input grayscale image**\n",
    "- The **reconstructed color image**\n",
    "- The **original ground truth color image**\n",
    "\n",
    "This helps us see how well the model is learning to colorize grayscale inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T19:24:23.631269Z",
     "iopub.status.busy": "2025-05-03T19:24:23.630995Z",
     "iopub.status.idle": "2025-05-03T19:24:24.414115Z",
     "shell.execute_reply": "2025-05-03T19:24:24.413316Z",
     "shell.execute_reply.started": "2025-05-03T19:24:23.631249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(n):\n",
    "    # Grayscale input\n",
    "    plt.subplot(n, 3, i*3 + 1)\n",
    "    plt.imshow(tf.squeeze(grayscale_imgs[i+35]), cmap='gray')\n",
    "    plt.title(\"Input (Gray)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Colorized output\n",
    "    plt.subplot(n, 3, i*3 + 2)\n",
    "    plt.imshow(predicted_color_imgs[i+35])\n",
    "    plt.title(\"Predicted (Color)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Ground truth color\n",
    "    plt.subplot(n, 3, i*3 + 3)\n",
    "    plt.imshow(real_color_imgs[i+35])\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4236041,
     "datasetId": 2465805,
     "sourceId": 4179266,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
